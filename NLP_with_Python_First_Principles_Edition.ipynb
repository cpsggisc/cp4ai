{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP with Python - First Principles Edition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqtAfhJN7pXmqO/1FORmn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhsgisc/cp4ai/blob/main/NLP_with_Python_First_Principles_Edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "- Tokenizing\n",
        "- Filtering Stop Words\n",
        "- Stemming\n",
        "- Tagging Parts of Speech\n",
        "- Lemmatizing\n",
        "- Chunking\n",
        "- Chinking\n",
        "- Using Named Entity Recognition (NER)\n",
        "- Getting Text to Analyze\n",
        "- Using a Concordance\n",
        "- Making a Dispersion Plot\n",
        "- Making a Frequency Distribution\n",
        "- Finding Collocations\n",
        "\n",
        "Reference: https://realpython.com/nltk-nlp-python/"
      ],
      "metadata": {
        "id": "1rUv2N2dbto1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3rolDbAbjgu",
        "outputId": "074133e0-2364-47ee-fd8f-0184e81f458a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"muad'dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training', 'was', 'in', 'how', 'to', 'learn', '', 'and', 'the', 'first', 'lesson', 'of', 'all', 'was', 'the', 'basic', 'trust', 'that', 'he', 'could', 'learn', '', \"it's\", 'shocking', 'to', 'find', 'how', 'many', 'people', 'do', 'not', 'believe', 'they', 'can', 'learn,', 'and', 'how', 'many', 'more', 'believe', 'learning', 'to', 'be', 'difficult']\n",
            "[\"muad'dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training', 'was', 'in', 'how', 'to', 'learn', 'and', 'the', 'lesson', 'of', 'all', 'basic', 'trust', 'that', 'he', 'could', \"it's\", 'shocking', 'find', 'many', 'people', 'do', 'not', 'believe', 'they', 'can', 'learn,', 'more', 'learning', 'be', 'difficult']\n",
            "37\n"
          ]
        }
      ],
      "source": [
        "# Tokenizing\n",
        "# break up text into sentences\n",
        "# break up sentence into words\n",
        "\n",
        "text = \"Muad'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"\n",
        "\n",
        "# a sentence ends with . ! ?\n",
        "terminators = ['.', '!', '?']\n",
        "\n",
        "# convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# replace terminators in sentence with period\n",
        "for terminator in terminators[1:]:\n",
        "  text = text.replace(terminator, terminator[0])\n",
        "  text = text.replace(terminator, terminator[0])\n",
        "\n",
        "# split text into sentence by period \n",
        "sentences = text.split('.')\n",
        "\n",
        "# remove last empty string \n",
        "sentences.pop()\n",
        "\n",
        "# split sentence into words by space\n",
        "words_all = []\n",
        "for sentence in sentences:\n",
        "  words_all.extend(sentence.split(' '))\n",
        "# words now contain duplicates including empty strings\n",
        "print(words_all)\n",
        "# remove duplicates\n",
        "words = []\n",
        "for word in words_all:\n",
        "  if word not in words:\n",
        "    words.append(word)\n",
        "# remove empty string\n",
        "words.remove('')    \n",
        "print(words)\n",
        "print(len(words))\n",
        "\n",
        "# alternatively, use set to remove all duplicates\n",
        "#print(len(words))\n",
        "#words = set(words)\n",
        "#print(len(words))\n",
        "#print(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenise(text):\n",
        "  # convert to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # replace terminators in sentence with period\n",
        "  text = text.replace('!', '.')\n",
        "  text = text.replace('?', '.')\n",
        "\n",
        "  # split text into sentence by period \n",
        "  sentences = text.split('.')\n",
        "\n",
        "  # remove last empty string \n",
        "  sentences.pop()\n",
        "\n",
        "  # split sentence into words by space\n",
        "  words_all = []\n",
        "  for sentence in sentences:\n",
        "    words_all.extend(sentence.split(' '))\n",
        "  # words now contain duplicates including empty strings\n",
        "  #print(words_all)\n",
        "  # remove duplicates\n",
        "  words = []\n",
        "  for word in words_all:\n",
        "    if word not in words:\n",
        "      words.append(word)\n",
        "  # remove empty string\n",
        "  if '' in words:\n",
        "    words.remove('')    \n",
        "  return words\n",
        "  #print(len(words)) \n",
        "\n",
        "# main\n",
        "text = \"Muad'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"\n",
        "words = tokenise(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnrqZccFqe64",
        "outputId": "6f0c9e29-fd8a-4723-ff1f-6079fbeb7aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"muad'dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training', 'was', 'in', 'how', 'to', 'learn', 'and', 'the', 'lesson', 'of', 'all', 'basic', 'trust', 'that', 'he', 'could', \"it's\", 'shocking', 'find', 'many', 'people', 'do', 'not', 'believe', 'they', 'can', 'learn,', 'more', 'learning', 'be', 'difficult']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering stop words\n",
        "stop_words = ['a', 'an', 'the'] # build up iteratively\n",
        "\n",
        "for word in words:\n",
        "  if word in stop_words:\n",
        "    words.remove(word)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq7j2dhWlXtx",
        "outputId": "1d578d61-b5c3-483c-fbb3-7f1479243275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"muad'dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training', 'was', 'in', 'how', 'to', 'learn', 'and', 'lesson', 'of', 'all', 'basic', 'trust', 'that', 'he', 'could', \"it's\", 'shocking', 'find', 'many', 'people', 'do', 'not', 'believe', 'they', 'can', 'learn,', 'more', 'learning', 'be', 'difficult']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stop_words(words):\n",
        "  stop_words = ['a', 'an', 'the'] # build up iteratively\n",
        "\n",
        "  for word in words:\n",
        "    if word in stop_words:\n",
        "      words.remove(word)\n",
        "  return words  "
      ],
      "metadata": {
        "id": "RlB9Ckj-p4w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "text_to_stem = \"The crew of the USS Discovery discovered many discoveries. Discovering is what explorers do.\"\n",
        "\n",
        "# convert to lowercase\n",
        "text_to_stem = text_to_stem.lower()\n",
        "\n",
        "words = tokenise(text_to_stem)\n",
        "words = filter_stop_words(words)\n",
        "\n",
        "# array\n",
        "#stemmer = ['learn', 'learns', 'learned', 'learnt', 'learner', 'learning']\n",
        "stemmer = ['discover', 'discovers', 'discovered', 'discovery', 'discoveries', 'discovering']\n",
        "stem = stemmer[0] # first item\n",
        "#for i in range(len(words)):\n",
        "#  if words[i] in stemmer:\n",
        "#    words[i] = stem\n",
        "#print(words)\n",
        "\n",
        "# dictionary\n",
        "# learn - learn, learned, learnt, learner, learning\n",
        "#stemmer_dict = {'learn': ['learn', 'learns', 'learned', 'learnt', 'learner', 'learning']}\n",
        "stemmer_dict = {'discover': ['discover', 'discovers', 'discovered', 'discovery', 'discoveries', 'discovering']}\n",
        "stem = list(stemmer_dict.keys())[0]\n",
        "for i in range(len(words)):\n",
        "  if words[i] in stemmer_dict[stem]:\n",
        "    words[i] = stem\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2E9nz4OnOYk",
        "outputId": "2ff75ab1-086f-4c02-d3d5-e840e650b865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'crew', 'of', 'the', 'uss', 'discovery', 'discovered', 'many', 'discoveries', '', 'discovering', 'is', 'what', 'explorers', 'do']\n",
            "['crew', 'of', 'uss', 'discover', 'discover', 'many', 'discover', 'discover', 'is', 'what', 'explorers', 'do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main \n",
        "words = tokenise(text_to_stem)\n",
        "words = filter_stop_words(words)\n",
        "words"
      ],
      "metadata": {
        "id": "4uovQ4iJrRHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tagging Parts of Speech\n",
        "'''\n",
        "JJ\tAdjectives\n",
        "NN\tNouns\n",
        "RB\tAdverbs\n",
        "PRP\tPronouns\n",
        "VB\tVerbs\n",
        "'''\n",
        "\n",
        "text_to_tag_pos = \"If you wish to make an apple pie from scratch, you must first invent the universe.\"\n",
        "\n",
        "words = tokenise(text_to_tag_pos)\n",
        "words = filter_stop_words(words)\n",
        "\n",
        "# array\n",
        "#pos_tags = []\n",
        "#pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'my', 'your', 'his', 'her', 'our', 'us', 'their']\n",
        "#tag = 'PRP'\n",
        "#for word in words:\n",
        "#  if word in pronouns:\n",
        "#    pos_tags.append([word, tag])\n",
        "#print(pos_tags)\n",
        "\n",
        "# dictionary\n",
        "pos_tags = []\n",
        "pronouns_dict = {'PRP': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'my', 'your', 'his', 'her', 'our', 'us', 'their']}\n",
        "tag = list(pronouns_dict.keys())[0]\n",
        "for word in words:\n",
        "  if word in pronouns_dict[tag]:\n",
        "    pos_tags.append([word, tag])\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBb3_bf6nU6w",
        "outputId": "e2134cf5-7247-4588-f6a2-9a54568e1616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRP\n",
            "[['you', 'PRP']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing\n",
        "# scarf - scarves\n",
        "# knife - knives\n",
        "# bad - worse, worst\n",
        "\n",
        "# array\n",
        "lemma = ['scarf', 'scarves']\n",
        "\n",
        "# dictionary\n",
        "lemma = {'scarf': 'scarves', 'knife': 'knives'}"
      ],
      "metadata": {
        "id": "Ibn5MXQgnX4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking\n"
      ],
      "metadata": {
        "id": "GI-sHPm4naXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chinking\n"
      ],
      "metadata": {
        "id": "AMZAbKimncgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Named Entity Recognition (NER)\n",
        "'''\n",
        "NE\t          Examples\n",
        "ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
        "PERSON\t      Eddy Bonte, President Obama\n",
        "LOCATION\t    Murray River, Mount Everest\n",
        "DATE\t        June, 2008-06-29\n",
        "TIME\t        two fifty a m, 1:30 p.m.\n",
        "MONEY\t        175 million Canadian dollars, GBP 10.40\n",
        "PERCENT\t      twenty pct, 18.75 %\n",
        "FACILITY\t    Washington Monument, Stonehenge\n",
        "GPE\t          South East Asia, Midlothian\n",
        "'''"
      ],
      "metadata": {
        "id": "XxumlFWfneRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Text to Analyze\n"
      ],
      "metadata": {
        "id": "sqffV19NngKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a Concordance\n"
      ],
      "metadata": {
        "id": "PPTtmcQ4nhzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a Dispersion Plot\n"
      ],
      "metadata": {
        "id": "fth3uLQXnjwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a Frequency Distribution\n",
        "# store words with their frequencies\n",
        "\n",
        "text = \"the quick brown fox jumps over the lazy dog\"\n",
        "words = text.split(' ')\n",
        "\n",
        "# array\n",
        "\n",
        "freqs = [0 for i in range(len(words))]\n",
        "for i in range(len(words)):\n",
        "  words[]\n",
        "\n",
        "# dictionary\n",
        "word_freq = {}\n",
        "\n",
        "\n",
        "\n",
        "# sort descending\n"
      ],
      "metadata": {
        "id": "FgvMZ9arnmMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Collocations\n",
        "# common word combinations\n",
        "# medium build; social drinker; non smoker; long term; would like; easy going; \n",
        "# financially secure; quiet night; well presented; never married; single mum; \n",
        "# permanent relationship; slim build; year old; similar interest; fun time; \n",
        "\n",
        "text = \"the quick brown fox jumps over the lazy dog\"\n",
        "words = text.split(' ')\n",
        "print(words)\n",
        "word_pairs = []\n",
        "for i in range(len(words)-1): # how many pairs?\n",
        "  word_pairs.append([words[i], words[i+1]])\n",
        "print(word_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zCibnoGnn5F",
        "outputId": "154095d4-0c43-46de-bd0f-113f86b6a3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "[['the', 'quick'], ['quick', 'brown'], ['brown', 'fox'], ['fox', 'jumps'], ['jumps', 'over'], ['over', 'the'], ['the', 'lazy'], ['lazy', 'dog']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"s1. s2? s3!\"\n",
        "s = s.replace('?', '.')\n",
        "s = s.replace('!', '.')\n",
        "print(s)\n",
        "lines = s.split('.')\n",
        "lines.pop()\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJi18QexfP5Q",
        "outputId": "e7c9eb16-1805-4213-914b-dc752f289310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s1. s2. s3.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s1', ' s2', ' s3']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'discover': ['discover', 'discovers', 'discovered', 'discovery', 'discoveries', 'discovering']}\n",
        "print(list(d.keys())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJF6vWPovdHJ",
        "outputId": "bd1146ff-b9c9-432b-effb-1ab09385fb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "discover\n"
          ]
        }
      ]
    }
  ]
}
